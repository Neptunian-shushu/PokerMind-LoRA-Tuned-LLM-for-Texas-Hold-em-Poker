{
  "episode": 300,
  "config": {
    "base_repo_or_path": "meta-llama/Meta-Llama-3-8B",
    "adapter_paths": [
      "/home/hice1/yli3776/PokerMind-LoRA-Tuned-LLM-for-Texas-Hold-em-Poker/poker-lora-model/Meta-Llama-3-8B"
    ],
    "adapter_register_names": [
      "A"
    ],
    "seat_adapter_names": [
      "A",
      "A"
    ],
    "output_dir": "/home/hice1/yli3776/PokerMind-LoRA-Tuned-LLM-for-Texas-Hold-em-Poker/ppo/ppo_checkpoints/",
    "log_dir": "/home/hice1/yli3776/PokerMind-LoRA-Tuned-LLM-for-Texas-Hold-em-Poker/logs/ppo/",
    "rl_adapter_save_dir": "/home/hice1/yli3776/PokerMind-LoRA-Tuned-LLM-for-Texas-Hold-em-Poker/ppo/rl_adapters/",
    "num_episodes": 1000,
    "steps_per_episode": 20,
    "learning_rate": 1e-05,
    "log_frequency": 5,
    "save_frequency": 50,
    "save_adapter_every": 1000,
    "eval_frequency": 50,
    "eval_episodes": 100,
    "num_players": 2,
    "starting_stack": 10.0,
    "small_blind": 0.5,
    "big_blind": 1.0,
    "max_seq_len": 512,
    "temperature": 0.8,
    "top_p": 0.9,
    "use_scoring": true,
    "device_map": "auto",
    "torch_dtype": "float16",
    "seed": 42,
    "max_grad_norm": 1.0
  },
  "last_log": {
    "steps": 1,
    "stacks_init": [
      10.0,
      10.0
    ],
    "stacks_final": [
      10.5,
      9.5
    ],
    "result": {
      "winners": [
        0
      ],
      "win_type": "fold",
      "pot": 1.5,
      "final_board": []
    },
    "terminal_rewards": [
      0.5,
      -0.5
    ]
  }
}