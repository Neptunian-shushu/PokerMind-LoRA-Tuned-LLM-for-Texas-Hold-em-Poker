{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc34b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da29849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GPU (CUDA): NVIDIA H100 80GB HBM3\n",
      "GPU Memory: 79.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check device availability in priority order (optimized for GPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using NVIDIA GPU (CUDA): {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41423cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded\n",
      "Train: 563200, Test: 11000\n"
     ]
    }
   ],
   "source": [
    "# Load the PokerBench dataset\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"RZ412/PokerBench\")\n",
    "    print(\"Dataset loaded\")\n",
    "    print(f\"Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b654f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "# IMPORTANT: Remove your token before pushing to GitHub!\n",
    "# Option 1: Use environment variable\n",
    "# login(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# Option 2: Use interactive login (will prompt for token)\n",
    "login()\n",
    "\n",
    "# Option 3: If you've already logged in once, you can just skip this cell\n",
    "# The token is cached in ~/.huggingface/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c770469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81aa4c4dcc1f45df8c08a1af79f741b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e5fab145754c46b2fb8e35785a9d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58888d9988144223b8cb9d5d750c569f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20de7b26fea74a4e8e4b875e9d94741d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b77814d1fa4f37826dddd7a99adda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9d5af42d8f46a6a158f75bbd968566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc9d3f450064b478bcb7d1b14fba0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1039932aaf854669b0ec17a60b2b8aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb07b7d833e49229dbe0a95210b48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb1e430fec9414095a7e8d8202c1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ae81b148594769af85172b2341368c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135855abb79a478497bd60eb19660cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7c043302e64b2cbef6ac156feda4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load two models\n",
    "MODEL_NAME_1 = \"google/gemma-2b\"\n",
    "MODEL_NAME_2 = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Load tokenizer and model for MODEL_NAME_1\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(MODEL_NAME_1)\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_1,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Load tokenizer and model for MODEL_NAME_2\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(MODEL_NAME_2)\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME_2,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad5d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format prompts (similar to poker_finetuning.ipynb)\n",
    "def format_prompt(instruction, output=\"\"):\n",
    "    \"\"\"Format instruction-output pairs for inference\"\"\"\n",
    "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "\n",
    "# Test on a single data point with reasoning/explanation\n",
    "def generate_single_response(model, tokenizer, instruction, max_length=200, include_reasoning=True):\n",
    "    \"\"\"Generate response for a single poker instruction\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for generation\n",
    "        tokenizer: The tokenizer to use\n",
    "        instruction: The poker scenario instruction\n",
    "        max_length: Maximum length of generated response\n",
    "        include_reasoning: If True, keeps explanations; if False, returns only action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt (leave output empty for generation)\n",
    "    prompt = format_prompt(instruction, \"\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with parameters optimized for reasoning\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + max_length,  # Add to input length\n",
    "            temperature=0.7,  # Higher temperature for more natural explanations\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part (remove the prompt)\n",
    "    generated_part = response[len(prompt):].strip()\n",
    "    \n",
    "    # If user doesn't want reasoning, extract just the first line (action only)\n",
    "    if not include_reasoning:\n",
    "        lines = generated_part.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                generated_part = line\n",
    "                break\n",
    "    \n",
    "    return generated_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb10560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are a specialist in playing 6-handed No Limit Texas Holdem. The following will be a game scenario and you need to make the optimal decision.\n",
      "\n",
      "Here is a game summary:\n",
      "\n",
      "The small blind is 0.5 chips and the big blind is 1 chips. Everyone started with 100 chips.\n",
      "The player positions involved in this game are UTG, HJ, CO, BTN, SB, BB.\n",
      "In this hand, your position is HJ, and your holding is [King of Diamond and Jack of Spade].\n",
      "Before the flop, HJ raise 2.0 chips, and BB call. Assume that all other players that is not mentioned folded.\n",
      "The flop comes King Of Spade, Seven Of Heart, and Two Of Diamond, then BB check, and HJ check.\n",
      "The turn comes Jack Of Club, then BB check, HJ bet 3 chips, BB raise 10 chips, and HJ call.\n",
      "The river comes Seven Of Club, then BB check.\n",
      "\n",
      "\n",
      "Now it is your turn to make a move.\n",
      "To remind you, the current pot size is 24.0 chips, and your holding is [King of Diamond and Jack of Spade].\n",
      "\n",
      "Decide on an action based on the strength of your hand on this board, your position, and actions before you. Do not explain your answer.\n",
      "Your optimal action is:\n"
     ]
    }
   ],
   "source": [
    "# Test the first model\n",
    "sample_input = dataset['train'][0]['instruction']\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82ee2aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for MODEL_NAME_2...\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for the train set using MODEL_NAME_2\n",
    "print(\"Generating responses for MODEL_NAME_2...\")\n",
    "responses_2 = generate_single_response(model_2, tokenizer_2, sample_input, include_reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9d8a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True response: bet 18\n",
      "Generated response: Check\n",
      "\n",
      "### Reasoning:\n",
      "There is no draw and my pair is not strong enough to call. I want to give up this hand and continue to play later.\n"
     ]
    }
   ],
   "source": [
    "print(\"True response:\", dataset['train'][0]['output'])\n",
    "print(\"Generated response:\", responses_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
