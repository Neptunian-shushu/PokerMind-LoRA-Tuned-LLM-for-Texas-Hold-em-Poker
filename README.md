# PokerMind: LoRA-Tuned LLM for Texas Hold'em Poker# PokerMind: LoRA-Tuned LLM for Texas Hold'em Poker



CS6220 Data and Visual Analytics Project - Georgia Institute of TechnologyThis repository contains the CS6220 (Data and Visual Analytics) project implementing LoRA fine-tuning of large language models for Texas Hold'em poker decision making, enhanced with PPO (Proximal Policy Optimization) self-play reinforcement learning.



A two-phase approach to training AI poker agents: **Supervised Fine-Tuning (SFT)** followed by **PPO Self-Play** reinforcement learning.## Project Overview



---This project uses a two-phase approach to train AI poker agents:

1. **Supervised Fine-Tuning (SFT)**: Fine-tune Llama-3-8B using LoRA on the PokerBench dataset (110k expert poker decisions)

## ğŸ¯ Project Overview2. **PPO Self-Play**: Further improve the model through reinforcement learning via self-play against itself



This project fine-tunes Meta-Llama-3-8B on poker decision-making using:The project achieves **35-45% action accuracy** after SFT and targets **55-65%+** accuracy after PPO training.



1. **Phase 1 - SFT**: Train on 110k expert poker hands from PokerBench dataset## Quick Start

   - âœ… **Complete**: 35-45% action accuracy achieved

   ### Phase 1: Supervised Fine-Tuning (âœ… COMPLETE)

2. **Phase 2 - PPO**: Improve through self-play reinforcement learning  ```bash

   - ğŸš§ **In Progress**: Target 55-65%+ accuracycd sft/

sbatch setup.sbatch  # Run on PACE ICE H100 GPU

---```



## ğŸ“ Project Structure**Results**: 35-45% action accuracy, model saved to `poker-lora-model/`



```### Phase 2: PPO Self-Play (ğŸš§ IN PROGRESS)

CS6220/```bash

â”œâ”€â”€ sft/                          # Supervised Fine-Tuning (âœ… Complete)cd ppo/

â”‚   â”œâ”€â”€ train_poker_model.py      # SFT training scriptpython train_ppo.py  # After implementation complete

â”‚   â””â”€â”€ setup.sbatch               # SLURM job for H100 GPU```

â”‚

â”œâ”€â”€ ppo/                          # PPO Reinforcement Learning (ğŸš§ Framework Ready)### Demo Interface (â³ PLANNED)

â”‚   â”œâ”€â”€ config.py                  # PPO hyperparameters```bash

â”‚   â”œâ”€â”€ rewards.py                 # Reward calculation with GAEcd demo/

â”‚   â”œâ”€â”€ agents.py                  # Agent wrappers (TODO)python app.py  # Play against trained AI

â”‚   â”œâ”€â”€ ppo_trainer.py             # PPO algorithm (TODO)```

â”‚   â””â”€â”€ train_ppo.py               # Main PPO training loop (TODO)

â”‚## Project Structure

â”œâ”€â”€ poker_game/                   # Texas Hold'em Game Engine (âœ… Complete)

â”‚   â”œâ”€â”€ game_logic.py              # Core poker game```

â”‚   â”œâ”€â”€ game_state.py              # State representationCS6220/

â”‚   â”œâ”€â”€ hand_evaluator.py          # Hand rankingâ”œâ”€â”€ sft/                          # Supervised Fine-Tuning

â”‚   â”œâ”€â”€ deck.py                    # Card managementâ”‚   â”œâ”€â”€ train_poker_model.py      # SFT training script

â”‚   â””â”€â”€ README.md                  # Game engine documentationâ”‚   â””â”€â”€ setup.sbatch               # SLURM job for H100 GPU

â”‚â”‚

â”œâ”€â”€ demo/                         # Interactive Demo (â³ Planned)â”œâ”€â”€ ppo/                          # PPO Reinforcement Learning

â”‚   â””â”€â”€ app.py                     # Gradio web interfaceâ”‚   â”œâ”€â”€ config.py                  # PPO hyperparameters

â”‚â”‚   â”œâ”€â”€ rewards.py                 # Reward calculation

â”œâ”€â”€ logs/                         # Training logsâ”‚   â”œâ”€â”€ agents.py                  # Agent wrappers (TODO)

â”‚   â”œâ”€â”€ sft/                       # SFT training outputsâ”‚   â”œâ”€â”€ ppo_trainer.py             # PPO algorithm (TODO)

â”‚   â””â”€â”€ ppo/                       # PPO training outputsâ”‚   â””â”€â”€ train_ppo.py               # Main PPO script (TODO)

â”‚â”‚

â”œâ”€â”€ poker-lora-model/             # Trained Modelsâ”œâ”€â”€ poker_game/                   # Reusable Game Engine

â”‚   â””â”€â”€ Meta-Llama-3-8B/          # SFT LoRA adapter weightsâ”‚   â”œâ”€â”€ game_logic.py              # Core poker game

â”‚â”‚   â”œâ”€â”€ game_state.py              # State representation

â””â”€â”€ requirements.txt              # Python dependenciesâ”‚   â”œâ”€â”€ hand_evaluator.py          # Hand ranking

```â”‚   â””â”€â”€ deck.py                    # Card management

â”‚

---â”œâ”€â”€ demo/                         # Interactive Demo (TODO)

â”‚   â””â”€â”€ app.py                     # Gradio interface

## ğŸš€ Quick Startâ”‚

â”œâ”€â”€ utils/                        # Shared utilities

### Phase 1: Supervised Fine-Tuning (âœ… Complete)â”‚   â””â”€â”€ (utility scripts)

â”‚

```bashâ”œâ”€â”€ logs/                         # Training logs

cd sft/â”‚   â”œâ”€â”€ sft/                       # SFT logs

sbatch setup.sbatch  # Run on PACE ICE H100 GPUâ”‚   â””â”€â”€ ppo/                       # PPO logs

```â”‚

â”œâ”€â”€ models/                       # Model checkpoints

**Results:**â””â”€â”€ poker-lora-model/             # Trained SFT model

- Preflop accuracy: 34.7%    â””â”€â”€ Meta-Llama-3-8B/          # LoRA adapter weights

- Postflop accuracy: 44.7%

- Model saved to `poker-lora-model/Meta-Llama-3-8B/````



### Phase 2: PPO Self-Play (ğŸš§ In Progress)See [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for detailed documentation.



```bash## Package Requirements

cd ppo/

python train_ppo.py  # After implementation completeCore dependencies include:

```- **PyTorch, Transformers, PEFT**: ML frameworks and fine-tuning

- **NumPy, Pandas, Matplotlib**: Data analysis and visualization

### Demo Interface (â³ Planned)- **Jupyter, Datasets, BitsAndBytes**: Development and optimization



```bashSee `requirements.txt` for complete list and `SETUP.md` for detailed installation guide.

cd demo/

python app.py  # Play against trained AI## Hardware Requirements

```

- **Minimum**: 8GB RAM, multi-core CPU

---- **Recommended**: 16GB+ RAM, NVIDIA GPU with 8GB+ VRAM

- **Storage**: 20GB+ free space

## ğŸ“Š Training Results

## Team Setup

### Supervised Fine-Tuning (SFT) - COMPLETE âœ…

For teammates to get started:

**Configuration:**1. Clone this repository

- Dataset: 110,000 poker hands (80k postflop + 30k preflop)2. Follow instructions in `SETUP.md`

- Model: Meta-Llama-3-8B with LoRA (r=16, Î±=32)3. Install packages from `requirements.txt`

- Training: 3 epochs on H100 80GB GPU4. Open and run `poker_finetuning.ipynb`

- Duration: ~4.6 hours

- Batch size: 128 (16 Ã— 8 accumulation)## Training Results



**Results:**### Supervised Fine-Tuning (SFT)

- Final loss: **0.2022** (89% improvement from 1.9058)- **Dataset**: 110,000 poker hands (80k postflop + 30k preflop)

- Train-val gap: **0.0006** (no overfitting âœ…)- **Training**: 3 epochs on H100 GPU (~4.6 hours)

- **Preflop**: 34.7% action accuracy- **Final Loss**: 0.2022 (89% improvement from 1.9058)

- **Postflop**: 44.7% action accuracy  - **Accuracy**: 

- **Overall**: ~42% exact match accuracy  - Preflop: 34.7%

  - Postflop: 44.7%

**Analysis:** This accuracy is expected for 3 epochs on complex poker decision-making. The model learned fundamental patterns and is ready for PPO enhancement.  - Overall: ~42% exact match



### PPO Self-Play - PLANNED ğŸš§**Analysis**: This accuracy is expected and normal for 3 epochs of training on complex poker decision-making. The model has learned fundamental poker patterns and is ready for PPO enhancement.



**Method:**### PPO Self-Play (Planned)

- Policy optimization with frozen reference model- **Method**: Policy optimization with frozen reference model

- KL divergence constraint (Î² = 0.1)- **Expected Improvement**: +15-20% accuracy (target 55-65%)

- 10,000-50,000 self-play games- **Training**: 10,000-50,000 self-play games

- **Key Innovation**: KL divergence constraint to prevent policy collapse

**Expected Results:**

- Target accuracy: **55-65%+** (+15-20% improvement)## Key Features

- Better strategic play (aggression, bluffing, adaptation)

- Win rate vs frozen opponent: 60%+âœ… **LoRA Fine-Tuning**: Memory-efficient training (only 0.2% params trainable)  

âœ… **4-bit Quantization**: Fits 8B model on single GPU  

---âœ… **Complete Game Engine**: Reusable Texas Hold'em implementation  

âœ… **Modular Architecture**: Separate modules for SFT, PPO, game logic, demo  

## ğŸ® Poker Game EngineğŸš§ **PPO Self-Play**: In development  

â³ **Interactive Demo**: Planned Gradio interface  

Complete Texas Hold'em implementation for PPO training and demo interface.

## Contributing

**Features:**

- âœ… Full game rules (blinds, betting, showdown)This is a course project for CS6220 at Georgia Tech. Team members should coordinate through the established communication channels.
- âœ… 2-10 player support
- âœ… Hand evaluation (High Card â†’ Straight Flush)
- âœ… Action validation (FOLD/CHECK/CALL/BET/RAISE)
- âœ… State tracking and history
- âœ… Reusable for training and demo

**Example:**
```python
from poker_game import PokerGame, Action

game = PokerGame(num_players=2, starting_stack=200.0)
state = game.reset()

# Get valid actions and execute
valid_actions = state.get_valid_actions(state.current_player())
state, done, result = game.step(Action.RAISE, amount=6.0)

if done:
    print(f"Winners: Player {result['winners']}")
```

See [`poker_game/README.md`](poker_game/README.md) for full documentation.

---

## ğŸ¤– PPO Framework

### Current Status

**Complete:**
- âœ… `config.py` - Hyperparameters (Î³=0.99, Îµ=0.2, KL=0.1)
- âœ… `rewards.py` - Reward calculation with GAE

**TODO:**
- â³ `agents.py` - Model wrappers (policy + reference)
- â³ `ppo_trainer.py` - PPO algorithm implementation
- â³ `train_ppo.py` - Training loop with self-play

### Reward Structure

```python
Win at showdown:  +1.0 + stack_change
Win by fold:      +1.0 + stack_change  
Lose:             -1.0 + stack_change
Fold penalty:     -0.1 (encourage playing)
Showdown bonus:   +0.2 (reward good play)
```

### Configuration

```python
from ppo.config import PPOConfig, PRODUCTION_CONFIG

# Production settings
config = PRODUCTION_CONFIG
# - 50k episodes
# - Batch size 64
# - Learning rate 5e-7
# - KL coefficient 0.1
```

---

## ğŸ”§ Technical Details

### Model Architecture
- **Base Model**: Meta-Llama-3-8B
- **Fine-tuning**: LoRA (r=16, alpha=32, dropout=0.1)
- **Quantization**: 4-bit NF4 with double quantization
- **Trainable Parameters**: ~17M / 8B (0.2%)

### Training Configuration

**SFT Phase:**
- Batch size: 128 (16 per device Ã— 8 accumulation)
- Learning rate: 1e-6 with cosine decay
- Optimizer: AdamW (weight_decay=0.01)
- Hardware: NVIDIA H100 80GB HBM3

**PPO Phase:**
- Episodes: 10,000-50,000 self-play games
- KL coefficient: 0.1 (stay close to SFT policy)
- Clip epsilon: 0.2 (standard PPO)
- Discount factor: 0.99
- GAE lambda: 0.95

---

## ğŸ’» Hardware Requirements

- **SFT Training**: H100 80GB or A100 40GB+
- **PPO Training**: H100 80GB recommended
- **Inference**: RTX 3090 24GB or similar
- **Demo**: Any GPU with 8GB+ VRAM

---

## ğŸ“¦ Dependencies

```bash
pip install -r requirements.txt
```

**Core packages:**
- PyTorch, Transformers, PEFT
- BitsAndBytes (quantization)
- Datasets, NumPy, Pandas
- Accelerate (distributed training)

---

## ğŸ¯ Next Steps

1. **Implement PPO Components**
   - [ ] Agent wrappers (`ppo/agents.py`)
   - [ ] PPO trainer (`ppo/ppo_trainer.py`)
   - [ ] Training script (`ppo/train_ppo.py`)

2. **Run PPO Training**
   - [ ] Test with 1k episodes
   - [ ] Full training with 10k-50k episodes
   - [ ] Evaluate vs frozen SFT opponent

3. **Build Demo Interface**
   - [ ] Gradio web UI
   - [ ] Human vs AI matches
   - [ ] Game state visualization

4. **Final Evaluation**
   - [ ] Test on held-out scenarios
   - [ ] Compare SFT vs PPO performance
   - [ ] Analyze learned strategies

---

## ğŸ“š Documentation

- **Main README**: This file - project overview and setup
- **[poker_game/README.md](poker_game/README.md)**: Game engine API and examples
- **[logs/sft/](logs/sft/)**: SFT training logs and analysis

---

## ğŸ”— Links

- **Repository**: [github.com/Neptunian-shushu/PokerMind-LoRA-Tuned-LLM-for-Texas-Hold-em-Poker](https://github.com/Neptunian-shushu/PokerMind-LoRA-Tuned-LLM-for-Texas-Hold-em-Poker)
- **Dataset**: [RZ412/PokerBench](https://huggingface.co/datasets/RZ412/PokerBench)
- **Base Model**: [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
- **Cluster**: PACE ICE (Georgia Tech)

---

## ğŸ“ Citation

```bibtex
@misc{pokermind2025,
  author = {bshu30},
  title = {PokerMind: LoRA-Tuned LLM for Texas Hold'em with PPO Self-Play},
  year = {2025},
  publisher = {Georgia Institute of Technology},
  journal = {CS6220 Course Project}
}
```

---

## ğŸ“§ Contact

- **GitHub Issues**: Open an issue on the repository
- **Email**: bshu30@gatech.edu

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Status**: Phase 1 (SFT) âœ… | Phase 2 (PPO) ğŸš§ | Phase 3 (Demo) â³
